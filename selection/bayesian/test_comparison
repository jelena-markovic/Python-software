import numpy as np
from scipy.stats import norm as ndist
from selection.tests.instance import gaussian_instance
from selection.bayesian.initial_soln import selection
from selection.bayesian.sel_probability import selection_probability
from selection.bayesian.non_scaled_sel_probability import no_scale_selection_probability
from selection.bayesian.sel_probability2 import cube_subproblem, cube_gradient, cube_barrier, selection_probability_objective
from selection.bayesian.barrier import barrier_conjugate_softmax, barrier_conjugate_log
from selection.bayesian.dual_optimization import selection_probability_dual_objective
from selection.randomized.api import randomization
from selection.bayesian.selection_probability import selection_probability_methods
#from selection.bayesian.objective_functions import selection_probability_only_objective
from selection.bayesian.dual_scipy import dual_selection_probability_func
from selection.bayesian.selective_map import bayesian_inference
from selection.bayesian.inference_rr import sel_prob_gradient_map, selective_map_credible
from selection.bayesian.approximation_based_intervals import approximate_conditional_sel_prob, \
    approximate_conditional_density
from matplotlib import pyplot as plt

#fixing n, p, true sparsity and signal strength
n = 10
p = 20
s = 3
snr = 5

#sampling the Gaussian instance
X_1, y, true_beta, nonzero, noise_variance = gaussian_instance(n=n, p=p, s=s, sigma=1, rho=0, snr=snr)
random_Z = np.random.standard_normal(p)
#getting randomized Lasso solution
sel = selection(X_1,y, random_Z)

#proceed only if selection is non-empty
if sel is not None:
    lam, epsilon, active, betaE, cube, initial_soln = sel
    print epsilon, lam, betaE
    noise_variance = 1
    nactive=betaE.shape[0]
    active_signs = np.sign(betaE)
    tau=1
    X_perm=np.zeros((n,p))
    X_perm[:,:nactive]=X_1[:,active]
    X_perm[:,nactive:]=X_1[:,~active]
    V=-X_perm
    X_active=X_perm[:,:nactive]
    X_nactive=X_perm[:,nactive:]
    B_sel=np.zeros((p,p))
    B_sel[:,:nactive]=np.dot(X_perm.T,X_perm[:,:nactive])
    B_sel[:nactive, :nactive]+= epsilon*np.identity(nactive)
    B_sel[nactive:, nactive:]=lam*np.identity((p-nactive))
    gamma_sel=np.zeros(p)
    gamma_sel[:nactive]=lam*np.sign(betaE)

    #box_initial = np.random.uniform(-1, 1,(p-nactive))
    #box_grad = np.true_divide(1,1-box_initial)-np.true_divide(1,1+box_initial)
    #coef_grad = -np.true_divide(1,betaE)+np.true_divide(1,1+betaE)
    #grad_barrier = np.append(coef_grad,box_grad)
    #dual_initial = np.dot(np.linalg.inv(B_sel.T),grad_barrier)
    feasible = np.append(-np.fabs(np.random.standard_normal(nactive)),np.random.uniform(-1, 1,(p-nactive)))
    dual_initial = np.dot(np.linalg.inv(B_sel.T),feasible)
    #print nactive, dual_initial

    parameter = np.ones(nactive)
    mean = X_1[:, active].dot(parameter)


    def test_selection_probability_gaussian():

        lagrange = lam * np.ones(p)

        sel_prob = selection_probability_objective(X_1,
                                                  np.fabs(betaE),
                                                  active,
                                                  active_signs,
                                                  lagrange,
                                                   mean,
                                                   noise_variance, randomization.isotropic_gaussian((p,), 1.),epsilon)
        return -sel_prob.minimize()[1],sel_prob.minimize()[0]


    def test_my_funct():

        sel = selection_probability(V, B_sel, gamma_sel, noise_variance, tau, lam, y, betaE, cube)

        return sel.optimization(parameter)[0]-np.true_divide(np.dot(
            mean.T,mean),2*noise_variance)-np.true_divide(np.dot(gamma_sel[:nactive].T,gamma_sel[:nactive]),2*(tau**2)),\
               sel.optimization(parameter)[1]

    def test_dual():

        lagrange = lam * np.ones(p)

        sel_dual = dual_selection_probability(X_1,
                                              dual_initial,
                                              active,
                                              active_signs,
                                              lagrange,
                                              mean,
                                              noise_variance,
                                              randomization.isotropic_gaussian((p,), 1.),
                                              epsilon)

        return sel_dual.minimize()[1]-np.true_divide(np.dot(mean.T,mean),2*noise_variance), sel_dual.minimize()[0]


    #print test_dual()

    #print test_selection_probability_gaussian(), test_my_funct()

    def test_one_sparse():
        if nactive==1:
            snr_seq = np.linspace(-10, 10, num=100)
            num = snr_seq.shape[0]
            #sel_non_scaled_seq = []
            sel_scaled_seq = []
            sel_grad_descent = []
            lagrange = lam * np.ones(p)
            for i in range(snr_seq.shape[0]):
                parameter = snr_seq[i]
                print "parameter value", parameter
                mean = X_1[:, active].dot(parameter)
                sel_non_scaled = no_scale_selection_probability(V, B_sel, gamma_sel, noise_variance, tau, lam, y, betaE, cube)
                sel_scaled= selection_probability(V, B_sel, gamma_sel, noise_variance, tau, lam, y, betaE, cube)

                sel_prob_grad_descent = selection_probability_objective(X_1, np.fabs(betaE), active, active_signs,
                                                                        lagrange, mean,
                                                                        noise_variance,
                                                                        randomization.isotropic_gaussian((p,), 1.),
                                                                        epsilon)

                #sel_non_scaled_val = sel_non_scaled.optimization(parameter * np.ones(nactive))[0] - np.true_divide(
                #    np.dot(mean.T, mean), 2 * noise_variance) - np.true_divide(
                #    np.dot(gamma_sel[:nactive].T, gamma_sel[:nactive]),2 * (tau ** 2))

                sel_scaled_prob = sel_scaled.optimization(parameter*np.ones(nactive),method="softmax_barrier")
                sel_scaled_prob_min = sel_scaled_prob[0]\
                                 -np.true_divide(np.dot(mean.T,mean),2*noise_variance)\
                                 -np.true_divide(np.dot(gamma_sel[:nactive].T,gamma_sel[:nactive]),2*(tau**2))
                                      #sel_scaled_prob[1]

                print "log selection probability", sel_scaled_prob_min, -sel_prob_grad_descent.minimize()[1]

                #sel_non_scaled_seq.append(sel_non_scaled_val)
                sel_scaled_seq.append(sel_scaled_prob_min)
                sel_grad_descent.append(-sel_prob_grad_descent.minimize()[1])

            #sel_non_scaled_seq = np.reshape(sel_non_scaled_seq, num)
            #sel_scaled_seq = np.reshape(sel_scaled_seq, num)
            #print np.shape(sel_non_scaled_seq), np.shape(sel_scaled_seq), np.shape(sel_grad_descent)

            #plt.clf()
            #plt.title("sel_prob")
            #plt.plot(snr_seq, sel_non_scaled_seq, color="b")
            #plt.plot(snr_seq, sel_scaled_seq, color="r")
            #plt.plot(snr_seq, sel_grad_descent, color="m")
            #plt.savefig('/Users/snigdhapanigrahi/Python_Plots/my_sel_prob.png')
            #plt.close()

    #test_one_sparse()

    def test_sel_probs_scalings():
        parameter = np.fabs(np.random.standard_normal(nactive))
        lagrange = lam * np.ones(p)
        mean = X_1[:, active].dot(parameter)
        sel_non_scaled = no_scale_selection_probability(V, B_sel, gamma_sel, noise_variance, tau, lam, y, betaE, cube)
        sel_scaled = selection_probability(V, B_sel, gamma_sel, noise_variance, tau, lam, y, betaE, cube)
        sel_prob_grad_descent = selection_probability_objective(X_1, np.fabs(betaE), active, active_signs,
                                                                lagrange, mean,
                                                                noise_variance,
                                                                randomization.isotropic_gaussian((p,), 1.),
                                                                epsilon)
        sel_non_scaled_val = sel_non_scaled.optimization(parameter * np.ones(nactive))[0] - np.true_divide(
                    np.dot(mean.T, mean), 2 * noise_variance) - np.true_divide(
                    np.dot(gamma_sel[:nactive].T, gamma_sel[:nactive]),2 * (tau ** 2))

        sel_scaled_val = sel_scaled.optimization(parameter * np.ones(nactive))[0] - np.true_divide(np.dot(
            mean.T, mean), 2 * noise_variance) - np.true_divide(np.dot(gamma_sel[:nactive].T, gamma_sel[:nactive]),
                                                                2 * (tau ** 2))

        print "log selection probability", sel_non_scaled_val, sel_scaled_val, -sel_prob_grad_descent.minimize()[1]


    #test_sel_probs_scalings()

    def test_sel_barrier_method():
        parameter = np.random.standard_normal(nactive)
        mean = X_1[:, active].dot(parameter)
        sel = selection_probability(V, B_sel, gamma_sel, noise_variance, tau, lam, y, betaE, cube)
        sel_val_log = sel.optimization(parameter * np.ones(nactive),method="log_barrier")[0] - np.true_divide(np.dot(
            mean.T, mean), 2 * noise_variance) - np.true_divide(np.dot(gamma_sel[:nactive].T, gamma_sel[:nactive]),
                                                                2 * (tau ** 2))
        sel_val_softmax = sel.optimization(parameter * np.ones(nactive),method="softmax_barrier")[0] - np.true_divide(np.dot(
            mean.T, mean), 2 * noise_variance) - np.true_divide(np.dot(gamma_sel[:nactive].T, gamma_sel[:nactive]),
                                                                2 * (tau ** 2))

        print "log selection probability", sel_val_log, sel_val_softmax


    #test_sel_barrier_method()

    def test_different_barriers():
        if nactive==1:
            snr_seq = np.linspace(0, 10, num=10)
            for i in range(snr_seq.shape[0]):
                parameter = snr_seq[i]
                mean = X_1[:, active].dot(parameter)
                print "parameter value", parameter
                sel = selection_probability(V, B_sel, gamma_sel, noise_variance, tau, lam, y, betaE, cube)
                sel_log_val = sel.optimization(parameter*np.ones(nactive),method="log_barrier")[0]-\
                              np.true_divide(np.dot(mean.T,mean),2*noise_variance)\
                              -np.true_divide(np.dot(gamma_sel[:nactive].T,gamma_sel[:nactive]),2*(tau**2))
                sel_softmax_val = sel.optimization(parameter*np.ones(nactive),method="softmax_barrier")[0]\
                                  -np.true_divide(np.dot(mean.T,mean),2*noise_variance)\
                                  -np.true_divide(np.dot(gamma_sel[:nactive].T,gamma_sel[:nactive]),2*(tau**2))
                print "log selection probability", sel_log_val, sel_softmax_val


    #test_different_barriers()

    def test_new_function():
        parameter = np.random.standard_normal(nactive)
        lagrange = lam * np.ones(p)
        mean = X_1[:, active].dot(parameter)
        sel = selection_probability(V, B_sel, gamma_sel, noise_variance, tau, lam, y, betaE, cube)

        sel_val_softmax = sel.optimization(parameter, method="softmax_barrier")
        sel_val = sel_val_softmax[0] - np.true_divide(np.dot(mean.T, mean), 2 * noise_variance) - \
                  np.true_divide(np.dot(gamma_sel[:nactive].T, gamma_sel[:nactive]),2 * (tau ** 2))

        sel_min = sel_val_softmax[1]

        sel_new = selection_probability_methods(X_1, np.fabs(betaE), active, active_signs,lagrange, mean,noise_variance,
                                                tau, epsilon)

        sel_val_new = sel_new.minimize_scipy()

        #sel_prob_grad_descent = selection_probability_objective(X_1, np.fabs(betaE), active, active_signs,
                                                                #lagrange, mean,
                                                                #noise_variance,
                                                                #randomization.isotropic_gaussian((p,), 1.),
                                                                #epsilon)

        print "value and minimizer--new", -sel_val_new[0], sel_val_new[1]

        print "value and minimizer", sel_val, sel_min

        #print "value and minimizer of original funct", -sel_prob_grad_descent.minimize()[1], \
            #sel_prob_grad_descent.minimize()[0]

    #test_new_function()


    def test_one_sparse():
        if nactive==1:
            snr_seq = np.linspace(-10, 10, num=100)
            num = snr_seq.shape[0]
            lagrange = lam * np.ones(p)
            for i in range(snr_seq.shape[0]):
                parameter = snr_seq[i]
                print "parameter value", parameter
                mean = X_1[:, active].dot(parameter)
                sel_new = selection_probability_methods(X_1, np.fabs(betaE), active, active_signs, lagrange, mean,
                                                        noise_variance,tau, epsilon)
                sel_val_new = sel_new.minimize_scipy()

                sel = selection_probability(V, B_sel, gamma_sel, noise_variance, tau, lam, y, betaE, cube)
                sel_val_softmax = sel.optimization(parameter*np.ones(nactive), method="softmax_barrier")
                sel_val = sel_val_softmax[0] - np.true_divide(np.dot(mean.T, mean), 2 * noise_variance) - \
                          np.true_divide(np.dot(gamma_sel[:nactive].T, gamma_sel[:nactive]), 2 * (tau ** 2))

                sel_min = sel_val_softmax[1]

                print "value and minimizer--new", -sel_val_new[0], sel_val_new[1]
                print "value and minimizer", sel_val, sel_min

    #test_one_sparse()

    def test_objectives():
        if nactive==1:
            snr_seq = np.linspace(-10, 10, num=100)
            num = snr_seq.shape[0]
            lagrange = lam * np.ones(p)
            for i in range(snr_seq.shape[0]):
                parameter = snr_seq[i]
                print "parameter value", parameter
                mean = X_1[:, active].dot(parameter)
                vec = np.random.standard_normal(n)
                active_coef = np.dot(np.diag(active_signs), np.fabs(np.random.standard_normal(nactive)))
                sel_prob_scipy = selection_probability_methods(X_1, np.fabs(betaE), active, active_signs, lagrange, mean,
                                                        noise_variance, tau, epsilon)
                sel_breakup_1 = sel_prob_scipy.objective(np.append(vec, np.fabs(active_coef)))
                sel_prob_grad =  selection_probability_only_objective(X_1, np.fabs(betaE), active, active_signs,
                                                                      lagrange, mean, noise_variance,
                                                                      randomization.isotropic_gaussian((p,), 1.),
                                                                      epsilon)
                sel_breakup_2 = sel_prob_grad.smooth_objective(np.append(vec,np.fabs(active_coef)),
                                                               mode='func', check_feasibility=False)
                print sel_breakup_2, sel_breakup_1


    #test_objectives()

    def test_two_optimizations():
        sel_prob_vec=[]
        sel_prob_vec_1=[]
        for i in range(10):
            parameter = np.random.standard_normal(nactive)
            lagrange = lam * np.ones(p)
            mean = X_1[:, active].dot(parameter)
            #print mean
            sel_prob_scipy = selection_probability_methods(X_1, np.fabs(betaE), active, active_signs, lagrange, mean,
                                                           noise_variance, tau, epsilon)

            sel_prob_scipy_val = sel_prob_scipy.minimize_scipy()

            sel_prob_scipy_val_1 = sel_prob_scipy.minimize_scipy_p()

            sel_prob_vec.append(sel_prob_scipy_val[0])

            sel_prob_vec_1.append(sel_prob_scipy_val_1[0])

            print -sel_prob_scipy_val[0], -sel_prob_scipy_val_1[0]

        #sel_prob_scipy_val_1 = sel_prob_scipy.objective_p(np.append(np.fabs(betaE),np.zeros(p-nactive)))
        #print np.asarray(sel_prob_vec), np.asarray(sel_prob_vec_1)
        #print "low dimensional", -sel_prob_scipy_val_1[0]
        #print "high dimensional", -sel_prob_scipy_val[0]


    #test_two_optimizations()

    def test_two_optimizations_one_sparse():

        if nactive == 1:
            snr_seq = np.linspace(-10, 10, num=100)
            lagrange = lam * np.ones(p)
            for i in range(snr_seq.shape[0]):
                parameter = snr_seq[i]
                print "parameter value", parameter
                mean = X_1[:, active].dot(parameter)
                sel_prob_scipy = selection_probability_methods(X_1, np.fabs(betaE), active, active_signs, lagrange, mean,
                                                       noise_variance, tau, epsilon)
                sel_prob_scipy_val = sel_prob_scipy.minimize_scipy()

                sel_prob_scipy_val_1 = sel_prob_scipy.minimize_scipy_p()
                print "low and high ", np.asscalar(-sel_prob_scipy_val_1[0]), -sel_prob_scipy_val[0]

    #test_two_optimizations_one_sparse()

    def test_dual_primal():

        dual_feasible = np.ones(p)
        dual_feasible[:nactive] = -np.fabs(np.random.standard_normal(nactive))

        for i in range(10):
            parameter = np.random.standard_normal(nactive)
            lagrange = lam * np.ones(p)
            mean = X_1[:, active].dot(parameter)
            #print mean
            sel_prob_scipy = selection_probability_methods(X_1, np.fabs(betaE), active, active_signs, lagrange, mean,
                                                           noise_variance, tau, epsilon)

            sel_dual_scipy = dual_selection_probability_func(X_1, dual_feasible, active, active_signs, lagrange, mean,
                                                           noise_variance, tau, epsilon)

            sel_prob_scipy_val = sel_prob_scipy.minimize_scipy()

            sel_prob_scipy_val_1 = sel_prob_scipy.minimize_scipy_p()

            sel_dual_scipy_val = sel_dual_scipy.minimize_dual()

            print -sel_prob_scipy_val[0], -sel_prob_scipy_val_1[0], sel_dual_scipy_val[0]


    #test_dual_primal()

    def test_dual_primal_one_sparse():

        if nactive == 1:
            snr_seq = np.linspace(-10, 10, num=20)
            lagrange = lam * np.ones(p)
            dual_feasible = np.ones(p)
            dual_feasible[:nactive] = -np.fabs(np.random.standard_normal(nactive))
            for i in range(snr_seq.shape[0]):
                parameter = snr_seq[i]
                mean = X_1[:, active].dot(parameter)

                sel_prob_scipy = selection_probability_methods(X_1, np.fabs(betaE), active, active_signs, lagrange,
                                                               mean,
                                                               noise_variance, tau, epsilon)

                sel_dual_scipy = dual_selection_probability_func(X_1, dual_feasible, active, active_signs, lagrange,
                                                                 mean,
                                                                 noise_variance, tau, epsilon)

                sel_prob_scipy_val = sel_prob_scipy.minimize_scipy()

                sel_prob_scipy_val_1 = sel_prob_scipy.minimize_scipy_p()

                sel_dual_scipy_val = sel_dual_scipy.minimize_dual()

                print -sel_prob_scipy_val[0], -sel_prob_scipy_val_1[0], sel_dual_scipy_val[0]


    #test_dual_primal_one_sparse()

    def compare_dual_objectives():

        dual_feasible = np.ones(p)
        dual_feasible[:nactive] = -np.fabs(np.random.standard_normal(nactive))

        for i in range(10):
            parameter = np.random.standard_normal(nactive)
            lagrange = lam * np.ones(p)
            mean = X_1[:, active].dot(parameter)
            test_point = np.ones(p)
            test_point[:nactive] = -np.fabs(np.random.standard_normal(nactive))

            #print mean
            sel_dual_scipy = dual_selection_probability_func(X_1, dual_feasible, active, active_signs, lagrange, mean,
                                                           noise_variance, tau, epsilon)

            sel_dual_regreg = selection_probability_dual_objective(X_1,
                                                                   dual_feasible,
                                                                   active,
                                                                   active_signs,
                                                                   lagrange,
                                                                   mean,
                                                                   noise_variance,
                                                                   randomization.isotropic_gaussian((p,), tau),
                                                                   epsilon)

            sel_dual_scipy_obj = sel_dual_scipy.dual_objective(test_point)

            sel_dual_regreg_obj = sel_dual_regreg.smooth_objective(test_point, mode='grad')

            #sel_dual_regreg_obj = sel_dual_regreg.objective_dual(test_point)

            print sel_dual_scipy_obj, sel_dual_regreg_obj

    #compare_dual_objectives()


    def compare_dual_minimizations():

        dual_feasible = np.ones(p)
        dual_feasible[:nactive] = -np.fabs(np.random.standard_normal(nactive))

        for i in range(10):
            parameter = np.random.standard_normal(nactive)
            lagrange = lam * np.ones(p)
            mean = X_1[:, active].dot(parameter)
            test_point = np.ones(p)
            test_point[:nactive] = -np.fabs(np.random.standard_normal(nactive))

            #print mean
            sel_dual_scipy = dual_selection_probability_func(X_1, dual_feasible, active, active_signs, lagrange, mean,
                                                           noise_variance, tau, epsilon)

            sel_dual_regreg = selection_probability_dual_objective(X_1,
                                                                   dual_feasible,
                                                                   active,
                                                                   active_signs,
                                                                   lagrange,
                                                                   mean,
                                                                   noise_variance,
                                                                   randomization.isotropic_gaussian((p,), tau),
                                                                   epsilon)

            sel_dual_scipy_obj = sel_dual_scipy.minimize_dual()

            sel_dual_regreg_obj = sel_dual_regreg.minimize2()

            #sel_dual_regreg_obj = sel_dual_regreg.objective_dual(test_point)

            print sel_dual_scipy_obj, sel_dual_regreg_obj


    #compare_dual_minimizations()

    def test_gradient():
        primal_feasible = np.fabs(betaE)
        dual_feasible = np.ones(p)
        dual_feasible[:nactive] = -np.fabs(np.random.standard_normal(nactive))
        lagrange = lam * np.ones(p)
        generative_X = X_1[:,active]
        prior_variance = 1
        inf = bayesian_inference(y,
                                 X_1,
                                 primal_feasible,
                                 dual_feasible,
                                 active,
                                 active_signs,
                                 lagrange,
                                 generative_X,
                                 noise_variance,
                                 prior_variance,
                                 randomization.isotropic_gaussian((p,), tau),
                                 epsilon)

        inf_rr = selective_map_credible(y,
                                        X_1,
                                        primal_feasible,
                                        dual_feasible,
                                        active,
                                        active_signs,
                                        lagrange,
                                        generative_X,
                                        noise_variance,
                                        prior_variance,
                                        randomization.isotropic_gaussian((p,), tau),
                                        epsilon)

        test = 10*np.random.standard_normal(nactive)
        #print inf.map_objective(test), inf_rr.smooth_objective(test, mode='func')
        #print -inf.log_prior(test), inf_rr.log_prior_loss.smooth_objective(test, mode='func')
        #print inf.likelihood(test), inf_rr.likelihood_loss.smooth_objective(test, mode='func')
        #print inf.gradient_selection_prob(test)[::-1][0], inf_rr.log_sel_prob.smooth_objective(test, mode='func')
        print -inf.gradient_posterior(test), -inf_rr.smooth_objective(test, mode='grad')
        #print inf_rr.map_solve_2()
        #print inf_rr.posterior_samples()

    test_gradient()
#######################################################################################################

n = 15
p = 10
s = 3
snr = 5


def posterior_samples_test(Langevin_steps=100):
    X_1, y, true_beta, nonzero, noise_variance = gaussian_instance(n=n, p=p, s=s, sigma=1, rho=0, snr=snr)
    random_Z = np.random.standard_normal(p)
    # getting randomized Lasso solution
    sel = selection(X_1, y, random_Z)
    lam, epsilon, active, betaE, cube, initial_soln = sel
    noise_variance = 1
    nactive = betaE.shape[0]
    active_signs = np.sign(betaE)
    tau = 1
    X_perm = np.zeros((n, p))
    X_perm[:, :nactive] = X_1[:, active]
    X_perm[:, nactive:] = X_1[:, ~active]
    B_sel = np.zeros((p, p))
    B_sel[:, :nactive] = np.dot(X_perm.T, X_perm[:, :nactive])
    B_sel[:nactive, :nactive] += epsilon * np.identity(nactive)
    B_sel[nactive:, nactive:] = lam * np.identity((p - nactive))
    gamma_sel = np.zeros(p)
    gamma_sel[:nactive] = lam * np.sign(betaE)
    primal_feasible = np.fabs(betaE)
    dual_feasible = np.ones(p)
    dual_feasible[:nactive] = -np.fabs(np.random.standard_normal(nactive))
    lagrange = lam * np.ones(p)
    generative_X = X_1[:, active]
    prior_variance = 1
    inf = bayesian_inference(y,
                             X_1,
                             primal_feasible,
                             dual_feasible,
                             active,
                             active_signs,
                             lagrange,
                             generative_X,
                             noise_variance,
                             prior_variance,
                             randomization.isotropic_gaussian((p,), tau),
                             epsilon)

    initial_condition = betaE
    gradient_map = lambda x: -inf.gradient_posterior(x)
    projection_map = lambda x: x
    stepsize = 1. / nactive
    state = initial_condition
    _noise = ndist(loc=0, scale=1)
    _sqrt_step = np.sqrt(stepsize)
    for i in range(Langevin_steps):
        proj_arg = state + 0.5 * stepsize * gradient_map(state) + _noise.rvs(initial_condition.shape[0]) * _sqrt_step
        candidate = projection_map(proj_arg)
        state = candidate
        print i, candidate, gradient_map(candidate)


#posterior_samples_test()



def approximate_ci_test():
    X_1, y, true_beta, nonzero, noise_variance = gaussian_instance(n=n, p=p, s=s, sigma=1, rho=0, snr=snr)
    random_Z = np.random.standard_normal(p)
    # getting randomized Lasso solution
    sel = selection(X_1, y, random_Z)
    lam, epsilon, active, betaE, cube, initial_soln = sel
    noise_variance = 1
    nactive = betaE.shape[0]
    active_signs = np.sign(betaE)
    tau = 1.
    X_perm = np.zeros((n, p))
    X_perm[:, :nactive] = X_1[:, active]
    X_perm[:, nactive:] = X_1[:, ~active]
    B_sel = np.zeros((p, p))
    B_sel[:, :nactive] = np.dot(X_perm.T, X_perm[:, :nactive])
    B_sel[:nactive, :nactive] += epsilon * np.identity(nactive)
    B_sel[nactive:, nactive:] = lam * np.identity((p - nactive))
    gamma_sel = np.zeros(p)
    gamma_sel[:nactive] = lam * np.sign(betaE)

    lagrange = lam * np.ones(p)
    feasible_point = np.fabs(betaE)

    truth = -1.

    approx = approximate_conditional_sel_prob(y,
                                              X_1,
                                              feasible_point,
                                              active,
                                              active_signs,
                                              lagrange,
                                              randomization.isotropic_gaussian((p,), tau),
                                              epsilon,
                                              j=1, #index of interest amongst active variables
                                              s= 2.)
    val = approx.smooth_objective(np.fabs(np.random.standard_normal(nactive)),mode='both')

    min = (approx.minimize(min_its=100, max_its=500, tol=1.e-10)[::-1])[0]

    cd = approximate_conditional_density(y,
                                         X_1,
                                         feasible_point,
                                         active,
                                         active_signs,
                                         lagrange,
                                         noise_variance,
                                         randomization.isotropic_gaussian((p,), tau),
                                         epsilon,
                                         j=1)

    #print cd.approx_conditional()

    print cd.approximate_ci()
        #, cd.approximate_ci()

    #print np.cumsum(cd.normalized_density(5.))

#approximate_ci_test()
















